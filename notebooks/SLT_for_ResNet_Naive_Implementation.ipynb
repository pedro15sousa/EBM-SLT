{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.autograd as autograd"
      ],
      "metadata": {
        "id": "-osJ0T-96Dci"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "        # Get the subnetwork by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort()\n",
        "        j = int((1 - k) * scores.numel())\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten()\n",
        "        flat_out[idx[:j]] = 0\n",
        "        flat_out[idx[j:]] = 1\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # send the gradient g straight-through on the backward pass.\n",
        "        return g, None\n",
        "\n",
        "class NonAffineBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, dim):\n",
        "        super(NonAffineBatchNorm, self).__init__(dim, affine=False)\n",
        "\n",
        "class SubnetConv(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "        self.prune_rate = prune_rate\n",
        "\n",
        "    def set_prune_rate(self, prune_rate):\n",
        "        self.prune_rate = prune_rate\n",
        "\n",
        "    @property\n",
        "    def clamped_scores(self):\n",
        "        return self.scores.abs()\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.clamped_scores, self.prune_rate)\n",
        "        w = self.weight * subnet\n",
        "        x = F.conv2d(\n",
        "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
        "        )\n",
        "        return x"
      ],
      "metadata": {
        "id": "HPv_Mb3oIrSS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "test_batch_size = 1000\n",
        "epochs = 20\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0005\n",
        "log_interval = 10\n",
        "data_path = \"data\"\n",
        "sparsity = 0.5\n",
        "save_model = True\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "last_layer_dense = True\n",
        "num_class = 10\n",
        "nonlinearity = \"relu\"\n",
        "init = \"kaiming_normal\"\n",
        "mode = \"fan_in\"\n",
        "scale_fan = False\n",
        "prune_rate = 0\n",
        "conv_type = SubnetConv\n",
        "bn_type = NonAffineBatchNorm\n",
        "first_layer_type = None\n",
        "first_layer_dense = True\n",
        "gpu = None"
      ],
      "metadata": {
        "id": "8DrcEakB_O1U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "neXky8oH5Fyz"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST(os.path.join(data_path, 'mnist'), train=True, download=True,\n",
        "                                                          transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                                                        transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                                           batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST(os.path.join(data_path, 'mnist'), train=False,\n",
        "                                                         transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                                                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                                          batch_size=test_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Builder(object):\n",
        "    def __init__(self, conv_layer, bn_layer, first_layer=None):\n",
        "        self.conv_layer = conv_layer\n",
        "        self.bn_layer = bn_layer\n",
        "        self.first_layer = first_layer or conv_layer\n",
        "\n",
        "\n",
        "    def conv(self, kernel_size, in_planes, out_planes, stride=1, first_layer=False):\n",
        "        conv_layer = self.first_layer if first_layer else self.conv_layer\n",
        "\n",
        "        if first_layer:\n",
        "            print(f\"==> Building first layer with {str(self.first_layer)}\")\n",
        "\n",
        "        if kernel_size == 3:\n",
        "            conv = conv_layer(\n",
        "                in_planes,\n",
        "                out_planes,\n",
        "                kernel_size=3,\n",
        "                stride=stride,\n",
        "                padding=1,\n",
        "                bias=False,\n",
        "            )\n",
        "        elif kernel_size == 1:\n",
        "            conv = conv_layer(\n",
        "                in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "            )\n",
        "        elif kernel_size == 5:\n",
        "            conv = conv_layer(\n",
        "                in_planes,\n",
        "                out_planes,\n",
        "                kernel_size=5,\n",
        "                stride=stride,\n",
        "                padding=2,\n",
        "                bias=False,\n",
        "            )\n",
        "        elif kernel_size == 7:\n",
        "            conv = conv_layer(\n",
        "                in_planes,\n",
        "                out_planes,\n",
        "                kernel_size=7,\n",
        "                stride=stride,\n",
        "                padding=3,\n",
        "                bias=False,\n",
        "            )\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        self._init_conv(conv)\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "    def conv3x3(self, in_planes, out_planes, stride=1, first_layer=False):\n",
        "        \"\"\"3x3 convolution with padding\"\"\"\n",
        "        c = self.conv(3, in_planes, out_planes, stride=stride, first_layer=first_layer)\n",
        "        return c\n",
        "\n",
        "    def conv1x1(self, in_planes, out_planes, stride=1, first_layer=False):\n",
        "        \"\"\"1x1 convolution with padding\"\"\"\n",
        "        c = self.conv(1, in_planes, out_planes, stride=stride, first_layer=first_layer)\n",
        "        return c\n",
        "\n",
        "    def conv7x7(self, in_planes, out_planes, stride=1, first_layer=False):\n",
        "        \"\"\"7x7 convolution with padding\"\"\"\n",
        "        c = self.conv(7, in_planes, out_planes, stride=stride, first_layer=first_layer)\n",
        "        return c\n",
        "\n",
        "    def conv5x5(self, in_planes, out_planes, stride=1, first_layer=False):\n",
        "        \"\"\"5x5 convolution with padding\"\"\"\n",
        "        c = self.conv(5, in_planes, out_planes, stride=stride, first_layer=first_layer)\n",
        "        return c\n",
        "\n",
        "    def batchnorm(self, planes, last_bn=False, first_layer=False):\n",
        "        return self.bn_layer(planes)\n",
        "\n",
        "    def activation(self):\n",
        "        if nonlinearity == \"relu\":\n",
        "            return (lambda: nn.ReLU(inplace=True))()\n",
        "        else:\n",
        "            raise ValueError(f\"{nonlinearity} is not an initialization option!\")\n",
        "\n",
        "    def _init_conv(self, conv):\n",
        "        if init == \"signed_constant\":\n",
        "\n",
        "            fan = nn.init._calculate_correct_fan(conv.weight, mode)\n",
        "            if scale_fan:\n",
        "                fan = fan * (1 - prune_rate)\n",
        "            gain = nn.init.calculate_gain(nonlinearity)\n",
        "            std = gain / math.sqrt(fan)\n",
        "            conv.weight.data = conv.weight.data.sign() * std\n",
        "\n",
        "        elif init == \"unsigned_constant\":\n",
        "\n",
        "            fan = nn.init._calculate_correct_fan(conv.weight, mode)\n",
        "            if scale_fan:\n",
        "                fan = fan * (1 - prune_rate)\n",
        "\n",
        "            gain = nn.init.calculate_gain(nonlinearity)\n",
        "            std = gain / math.sqrt(fan)\n",
        "            conv.weight.data = torch.ones_like(conv.weight.data) * std\n",
        "\n",
        "        elif init == \"kaiming_normal\":\n",
        "\n",
        "            if scale_fan:\n",
        "                fan = nn.init._calculate_correct_fan(conv.weight, mode)\n",
        "                fan = fan * (1 - prune_rate)\n",
        "                gain = nn.init.calculate_gain(nonlinearity)\n",
        "                std = gain / math.sqrt(fan)\n",
        "                with torch.no_grad():\n",
        "                    conv.weight.data.normal_(0, std)\n",
        "            else:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    conv.weight, mode=mode, nonlinearity=nonlinearity\n",
        "                )\n",
        "\n",
        "        elif init == \"kaiming_uniform\":\n",
        "            nn.init.kaiming_uniform_(\n",
        "                conv.weight, mode=mode, nonlinearity=nonlinearity\n",
        "            )\n",
        "        elif init == \"xavier_normal\":\n",
        "            nn.init.xavier_normal_(conv.weight)\n",
        "        elif init == \"xavier_constant\":\n",
        "\n",
        "            fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(conv.weight)\n",
        "            std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
        "            conv.weight.data = conv.weight.data.sign() * std\n",
        "\n",
        "        elif init == \"standard\":\n",
        "\n",
        "            nn.init.kaiming_uniform_(conv.weight, a=math.sqrt(5))\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"{init} is not an initialization option!\")\n",
        "\n",
        "\n",
        "def get_builder():\n",
        "\n",
        "    print(\"==> Conv Type: {}\".format(conv_type))\n",
        "    print(\"==> BN Type: {}\".format(bn_type))\n",
        "\n",
        "    #conv_layer = getattr(utils.conv_type, conv_type)\n",
        "    #bn_layer = getattr(utils.bn_type, bn_type)\n",
        "    conv_layer = conv_type\n",
        "    bn_layer = bn_type\n",
        "\n",
        "    if first_layer_type is not None:\n",
        "        first_layer = getattr(conv_type, first_layer_type)\n",
        "        print(f\"==> First Layer Type: {first_layer_type}\")\n",
        "    else:\n",
        "        first_layer = None\n",
        "\n",
        "    builder = Builder(conv_layer=conv_layer, bn_layer=bn_layer, first_layer=first_layer)\n",
        "    #builder = Builder(conv_layer=None, bn_layer=None, first_layer=None)\n",
        "\n",
        "    return builder"
      ],
      "metadata": {
        "id": "g8Xuc3QOEeRp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    #M = 2\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, builder, inplanes, planes, stride=1, downsample=None, base_width=64):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if base_width / 64 > 1:\n",
        "            raise ValueError(\"Base width >64 does not work for BasicBlock\")\n",
        "\n",
        "        self.conv1 = builder.conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = builder.batchnorm(planes)\n",
        "        self.relu = builder.activation()\n",
        "        self.conv2 = builder.conv3x3(planes, planes)\n",
        "        self.bn2 = builder.batchnorm(planes, last_bn=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        if self.bn1 is not None:\n",
        "            out = self.bn1(out)\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.bn2 is not None:\n",
        "            out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    #M = 3\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, builder, inplanes, planes, stride=1, downsample=None, base_width=64):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        width = int(planes * base_width / 64)\n",
        "        self.conv1 = builder.conv1x1(inplanes, width)\n",
        "        self.bn1 = builder.batchnorm(width)\n",
        "        self.conv2 = builder.conv3x3(width, width, stride=stride)\n",
        "        self.bn2 = builder.batchnorm(width)\n",
        "        self.conv3 = builder.conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = builder.batchnorm(planes * self.expansion, last_bn=True)\n",
        "        self.relu = builder.activation()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, builder, block, layers, num_classes=1000, base_width=64):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.base_width = base_width\n",
        "        if self.base_width // 64 > 1:\n",
        "            print(f\"==> Using {self.base_width // 64}x wide model\")\n",
        "\n",
        "        if first_layer_dense:\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = builder.conv7x7(3, 64, stride=2, first_layer=True)\n",
        "\n",
        "        self.bn1 = builder.batchnorm(64)\n",
        "        self.relu = builder.activation()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(builder, block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(builder, block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(builder, block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(builder, block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        if last_layer_dense:\n",
        "            self.fc = nn.Conv2d(512 * block.expansion, num_classes, 1)\n",
        "        else:\n",
        "            self.fc = builder.conv1x1(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, builder, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            dconv = builder.conv1x1(\n",
        "                self.inplanes, planes * block.expansion, stride=stride\n",
        "            )\n",
        "            dbn = builder.batchnorm(planes * block.expansion)\n",
        "            if dbn is not None:\n",
        "                downsample = nn.Sequential(dconv, dbn)\n",
        "            else:\n",
        "                downsample = dconv\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(builder, self.inplanes, planes, stride, downsample, base_width=self.base_width))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(builder, self.inplanes, planes, base_width=self.base_width))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if self.bn1 is not None:\n",
        "            x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "EkyHH0nbDb5f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, writer = None):\n",
        "    # batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
        "    # data_time = AverageMeter(\"Data\", \":6.3f\")\n",
        "    # losses = AverageMeter(\"Loss\", \":.3f\")\n",
        "    # top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
        "    # top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
        "    # progress = ProgressMeter(len(train_loader),[batch_time, data_time, losses, top1, top5],prefix=f\"Epoch: [{epoch}]\",)\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    batch_size = train_loader.batch_size\n",
        "    num_batches = len(train_loader)\n",
        "    end = time.time()\n",
        "    for i, (images, target) in tqdm(enumerate(train_loader), ascii=True, total=len(train_loader)):\n",
        "        # measure data loading time\n",
        "        # data_time.update(time.time() - end)\n",
        "\n",
        "        if gpu is not None:\n",
        "            images = images.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        # print(output.shape)\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        # losses.update(loss.item(), images.size(0))\n",
        "        # top1.update(acc1.item(), images.size(0))\n",
        "        # top5.update(acc5.item(), images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        # batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # if i % args.print_freq == 0:\n",
        "            # t = (num_batches * epoch + i) * batch_size\n",
        "            # progress.display(i)\n",
        "            # progress.write_to_tensorboard(writer, prefix=\"train\", global_step=t)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, writer = None, epoch = None):\n",
        "    # batch_time = AverageMeter(\"Time\", \":6.3f\", write_val=False)\n",
        "    # losses = AverageMeter(\"Loss\", \":.3f\", write_val=False)\n",
        "    # top1 = AverageMeter(\"Acc@1\", \":6.2f\", write_val=False)\n",
        "    # top5 = AverageMeter(\"Acc@5\", \":6.2f\", write_val=False)\n",
        "    # progress = ProgressMeter(\n",
        "    #     len(val_loader), [batch_time, losses, top1, top5], prefix=\"Test: \")\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, target) in tqdm(enumerate(val_loader), ascii=True, total=len(val_loader)):\n",
        "            if gpu is not None:\n",
        "                images = images.to(device)\n",
        "\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            # print(output.shape)\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            # losses.update(loss.item(), images.size(0))\n",
        "            # top1.update(acc1.item(), images.size(0))\n",
        "            # top5.update(acc5.item(), images.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            # batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # if i % args.print_freq == 0:\n",
        "                # progress.display(i)\n",
        "\n",
        "        # progress.display(len(val_loader))\n",
        "\n",
        "        # if writer is not None:\n",
        "            # progress.write_to_tensorboard(writer, prefix=\"test\", global_step=epoch)\n",
        "\n",
        "    return acc1, acc5\n"
      ],
      "metadata": {
        "id": "5G3611E8Kuou"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(get_builder(), Bottleneck, [3, 4, 23, 3], 10).to(device)\n",
        "\n",
        "optimizer = optim.SGD([p for p in model.parameters() if p.requires_grad],\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "acc1, acc5 = validate(test_loader, model, criterion)\n",
        "print(acc1, acc5)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    acc1, acc5 = validate(test_loader, model, criterion)\n",
        "    print(loss, acc1, acc5)\n",
        "    scheduler.step()\n",
        "\n",
        "if save_model:\n",
        "    torch.save(model.state_dict(), \"mnist_res50.pt\")"
      ],
      "metadata": {
        "id": "ZzfHTwdK_Ys8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}